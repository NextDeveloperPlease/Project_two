{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52b56a26-dc6a-43bb-8398-a76752038969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 134,198\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>statement</th>\n",
       "      <th>target</th>\n",
       "      <th>BinaryNumTarget</th>\n",
       "      <th>manual_keywords</th>\n",
       "      <th>tweet</th>\n",
       "      <th>5_label_majority_answer</th>\n",
       "      <th>3_label_majority_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93848</th>\n",
       "      <td>93848</td>\n",
       "      <td>Eric Stirgus</td>\n",
       "      <td>Sixteen million jobs were created under Ronald...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16 million, jobs, created, Reagan</td>\n",
       "      <td>@BillOReilly Trump lost more jobs in 4 years t...</td>\n",
       "      <td>NO MAJORITY</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73158</th>\n",
       "      <td>73158</td>\n",
       "      <td>Louis Jacobson</td>\n",
       "      <td>Polls show that Americans \"overwhelmingly\" sup...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Americans support, expanding background checks</td>\n",
       "      <td>@GSPLewie @politiCOHEN_ 84% of Americans\\r\\nsu...</td>\n",
       "      <td>NO MAJORITY</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52174</th>\n",
       "      <td>52174</td>\n",
       "      <td>Tom Kertscher</td>\n",
       "      <td>In the 2020 presidential election, 4,255 ballo...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Fulton County, election, ballots multiple scanned</td>\n",
       "      <td>@GeorgiaFootball Former Senator David Perdue S...</td>\n",
       "      <td>Mostly Agree</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72643</th>\n",
       "      <td>72643</td>\n",
       "      <td>Louis Jacobson</td>\n",
       "      <td>\"Nearly 6 out of 10 believe that money and wea...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>money, more evenly distributed</td>\n",
       "      <td>@ladiesm87780436 Im hoping this is a reverse  ...</td>\n",
       "      <td>Mostly Agree</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26924</th>\n",
       "      <td>26924</td>\n",
       "      <td>Lauren Carroll</td>\n",
       "      <td>\"Trump Management was charged with discriminat...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Trump Management, African-Americans</td>\n",
       "      <td>@TimMurtaugh @DonaldJTrumpJr @nytimes @realDon...</td>\n",
       "      <td>Mostly Disagree</td>\n",
       "      <td>Disagree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86896</th>\n",
       "      <td>86896</td>\n",
       "      <td>Tom Kertscher</td>\n",
       "      <td>Donald J. Trumps signature is on the new stimu...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>trump,signature,stimulus checks</td>\n",
       "      <td>@Not2dayeva @RubyRoseTHC Do you mean how Trump...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38835</th>\n",
       "      <td>38835</td>\n",
       "      <td>Lauren Caruba</td>\n",
       "      <td>\"More black babies are aborted in NYC than born.\"</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NYC, black babies, aborted</td>\n",
       "      <td>@Kuntrella @softwaredev73 @Liz_Wheeler @Meghan...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33020</th>\n",
       "      <td>33020</td>\n",
       "      <td>Will Doran</td>\n",
       "      <td>Says Donald Trump \"wants to get rid of the fed...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Trump, federal minimum wage</td>\n",
       "      <td>poor people who voted for (trump)\\r\\nsaw their...</td>\n",
       "      <td>Mostly Disagree</td>\n",
       "      <td>Disagree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21805</th>\n",
       "      <td>21805</td>\n",
       "      <td>Miriam Valverde</td>\n",
       "      <td>Illegal immigrationon the U.S.-Mexico border i...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Illegal immigration, lowest, 17 years</td>\n",
       "      <td>Illegal Immigration Apprehensions Drop to The ...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72514</th>\n",
       "      <td>72514</td>\n",
       "      <td>Louis Jacobson</td>\n",
       "      <td>\"Nearly 6 out of 10 believe that money and wea...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>money, more evenly distributed</td>\n",
       "      <td>@Acaicia2 @RobinToal @vPingle @elonmusk throwi...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134198 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0           author  \\\n",
       "93848       93848     Eric Stirgus   \n",
       "73158       73158   Louis Jacobson   \n",
       "52174       52174    Tom Kertscher   \n",
       "72643       72643   Louis Jacobson   \n",
       "26924       26924   Lauren Carroll   \n",
       "...           ...              ...   \n",
       "86896       86896    Tom Kertscher   \n",
       "38835       38835    Lauren Caruba   \n",
       "33020       33020       Will Doran   \n",
       "21805       21805  Miriam Valverde   \n",
       "72514       72514   Louis Jacobson   \n",
       "\n",
       "                                               statement  target  \\\n",
       "93848  Sixteen million jobs were created under Ronald...    True   \n",
       "73158  Polls show that Americans \"overwhelmingly\" sup...    True   \n",
       "52174  In the 2020 presidential election, 4,255 ballo...   False   \n",
       "72643  \"Nearly 6 out of 10 believe that money and wea...    True   \n",
       "26924  \"Trump Management was charged with discriminat...    True   \n",
       "...                                                  ...     ...   \n",
       "86896  Donald J. Trumps signature is on the new stimu...   False   \n",
       "38835  \"More black babies are aborted in NYC than born.\"    True   \n",
       "33020  Says Donald Trump \"wants to get rid of the fed...    True   \n",
       "21805  Illegal immigrationon the U.S.-Mexico border i...    True   \n",
       "72514  \"Nearly 6 out of 10 believe that money and wea...    True   \n",
       "\n",
       "       BinaryNumTarget                                    manual_keywords  \\\n",
       "93848              1.0                  16 million, jobs, created, Reagan   \n",
       "73158              1.0     Americans support, expanding background checks   \n",
       "52174              0.0  Fulton County, election, ballots multiple scanned   \n",
       "72643              1.0                     money, more evenly distributed   \n",
       "26924              1.0                Trump Management, African-Americans   \n",
       "...                ...                                                ...   \n",
       "86896              0.0                    trump,signature,stimulus checks   \n",
       "38835              1.0                         NYC, black babies, aborted   \n",
       "33020              1.0                        Trump, federal minimum wage   \n",
       "21805              1.0              Illegal immigration, lowest, 17 years   \n",
       "72514              1.0                     money, more evenly distributed   \n",
       "\n",
       "                                                   tweet  \\\n",
       "93848  @BillOReilly Trump lost more jobs in 4 years t...   \n",
       "73158  @GSPLewie @politiCOHEN_ 84% of Americans\\r\\nsu...   \n",
       "52174  @GeorgiaFootball Former Senator David Perdue S...   \n",
       "72643  @ladiesm87780436 Im hoping this is a reverse  ...   \n",
       "26924  @TimMurtaugh @DonaldJTrumpJr @nytimes @realDon...   \n",
       "...                                                  ...   \n",
       "86896  @Not2dayeva @RubyRoseTHC Do you mean how Trump...   \n",
       "38835  @Kuntrella @softwaredev73 @Liz_Wheeler @Meghan...   \n",
       "33020  poor people who voted for (trump)\\r\\nsaw their...   \n",
       "21805  Illegal Immigration Apprehensions Drop to The ...   \n",
       "72514  @Acaicia2 @RobinToal @vPingle @elonmusk throwi...   \n",
       "\n",
       "      5_label_majority_answer 3_label_majority_answer  \n",
       "93848             NO MAJORITY                   Agree  \n",
       "73158             NO MAJORITY                   Agree  \n",
       "52174            Mostly Agree                   Agree  \n",
       "72643            Mostly Agree                   Agree  \n",
       "26924         Mostly Disagree                Disagree  \n",
       "...                       ...                     ...  \n",
       "86896                   Agree                   Agree  \n",
       "38835                   Agree                   Agree  \n",
       "33020         Mostly Disagree                Disagree  \n",
       "21805                   Agree                   Agree  \n",
       "72514                   Agree                   Agree  \n",
       "\n",
       "[134198 rows x 9 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATASET_PATH = \"Truth_Seeker_Model_Dataset.csv\"\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "df = df.sample(frac=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8da093e7-2473-4402-963e-7205d4adac55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111593, 9)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(df[df['5_label_majority_answer'] == 'NO MAJORITY'].index, axis=0)\n",
    "df = df.drop(df[df['3_label_majority_answer'] == 'NO MAJORITY'].index, axis=0)\n",
    "#labels = df[\"BinaryNumTarget\"].values\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59cffe6a-71d1-425e-9f78-a089d4e5e7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = 'Statement: ' + df['statement'] + '| Tweet: ' +df['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13803549-d9c5-4d3f-8d3c-c496cba66ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Statement: End of eviction moratorium means millions of Americans could lose their housing in the middle of a pandemic.| Tweet: @POTUS Biden Blunders - 6 Month Update\\r\\n\\r\\nInflation, Delta mismanagement, COVID for kids, Abandoning Americans in Afghanistan, Arming the Taliban, S. Border crisis, Breaking job growth, Abuse of power (Many Exec Orders, $3.5T through Reconciliation, Eviction Moratorium)...what did I miss?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a7ec289-2593-4715-8124-6fb1179de6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 111593 entries, 52174 to 72514\n",
      "Data columns (total 9 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   Unnamed: 0               111593 non-null  int64  \n",
      " 1   author                   111593 non-null  object \n",
      " 2   statement                111593 non-null  object \n",
      " 3   target                   111593 non-null  bool   \n",
      " 4   BinaryNumTarget          111593 non-null  float64\n",
      " 5   manual_keywords          111593 non-null  object \n",
      " 6   tweet                    111593 non-null  object \n",
      " 7   5_label_majority_answer  111593 non-null  object \n",
      " 8   3_label_majority_answer  111593 non-null  object \n",
      "dtypes: bool(1), float64(1), int64(1), object(6)\n",
      "memory usage: 11.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53567947-0e0f-4120-8a9b-92c070809b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_truthfulness_4way(row):\n",
    "    if row['target'] == True:\n",
    "        if row['5_label_majority_answer'] == 'Agree':\n",
    "            return \"True\"\n",
    "        elif row['5_label_majority_answer'] == 'Disagree':\n",
    "            return \"False\"\n",
    "        elif row['5_label_majority_answer'] == 'Mostly Agree':\n",
    "            return \"True\"\n",
    "        elif row['5_label_majority_answer'] == 'Mostly Disagree':\n",
    "            return \"False\"\n",
    "    else:\n",
    "        if row['5_label_majority_answer'] == 'Agree':\n",
    "            return \"False\"\n",
    "        elif row['5_label_majority_answer'] == 'Disagree':\n",
    "            return \"True\"\n",
    "        elif row['5_label_majority_answer'] == 'Mostly Agree':\n",
    "            return \"False\"\n",
    "        elif row['5_label_majority_answer'] == 'Mostly Disagree':\n",
    "            return \"True\"\n",
    "    print(row)\n",
    "    return None\n",
    "\n",
    "def generate_truthfulness_2way(row):\n",
    "    if row['target'] == True:\n",
    "        if row['3_label_majority_answer'] == 'Agree':\n",
    "            return \"True\"\n",
    "        elif row['3_label_majority_answer'] == 'Disagree':\n",
    "            return \"False\"\n",
    "    else:\n",
    "        if row['3_label_majority_answer'] == 'Agree':\n",
    "            return \"False\"\n",
    "        elif row['3_label_majority_answer'] == 'Disagree':\n",
    "            return \"True\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6af7d7a7-da69-45f3-a5c2-054e4e63ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame()\n",
    "df2['4-way-label'] = df.apply(lambda x: generate_truthfulness_4way(x), axis=1)\n",
    "df2['2-way-label'] = df.apply(lambda x: generate_truthfulness_2way(x), axis=1)\n",
    "#df2.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af0b815c-bc6b-4e8a-bbfc-74034db88b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111593, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24f91527-667e-4278-8f96-d58d438f7e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_39624\\2916450262.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df2['2-way-label'] = df2['2-way-label'].replace({'True': 0, 'False': 1})\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_39624\\2916450262.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df2['4-way-label'] = df2['4-way-label'].replace({'True': 0, 'False': 1})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(111593,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['2-way-label'] = df2['2-way-label'].replace({'True': 0, 'False': 1})\n",
    "df2['4-way-label'] = df2['4-way-label'].replace({'True': 0, 'False': 1})\n",
    "labels = df2[\"4-way-label\"].values\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5affeb06-8cd1-4b58-9000-09419efac91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4-way-label</th>\n",
       "      <th>2-way-label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52174</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72643</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26924</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5123</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119269</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86896</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38835</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33020</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21805</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72514</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111593 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        4-way-label  2-way-label\n",
       "52174             1            1\n",
       "72643             0            0\n",
       "26924             1            1\n",
       "5123              0            0\n",
       "119269            1            1\n",
       "...             ...          ...\n",
       "86896             1            1\n",
       "38835             0            0\n",
       "33020             1            1\n",
       "21805             0            0\n",
       "72514             0            0\n",
       "\n",
       "[111593 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15df6ee4-c79f-478a-b893-f5c29195ed7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        4-way-label  2-way-label\n",
      "52174             1            1\n",
      "72643             0            0\n",
      "26924             1            1\n",
      "5123              0            0\n",
      "119269            1            1\n",
      "...             ...          ...\n",
      "86896             1            1\n",
      "38835             0            0\n",
      "33020             1            1\n",
      "21805             0            0\n",
      "72514             0            0\n",
      "\n",
      "[111593 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df2['2-way-label'].plot(kind='hist')\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cefbc76-0e2c-4afc-bc90-c1cd2c172cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((111593,), (111593, 2))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df2['2-way-label'].values\n",
    "labels.shape, df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01944a71-51db-47e2-b7f8-41fe07d1609b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5031cf4e-3047-4bdd-99d7-90d1f2fb2d87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  Statement: End of eviction moratorium means millions of Americans could lose their housing in the middle of a pandemic.| Tweet: @POTUS Biden Blunders - 6 Month Update\n",
      "\n",
      "Inflation, Delta mismanagement, COVID for kids, Abandoning Americans in Afghanistan, Arming the Taliban, S. Border crisis, Breaking job growth, Abuse of power (Many Exec Orders, $3.5T through Reconciliation, Eviction Moratorium)...what did I miss?\n",
      "Tokenized:  ['statement', ':', 'end', 'of', 'ev', '##iction', 'mora', '##torium', 'means', 'millions', 'of', 'americans', 'could', 'lose', 'their', 'housing', 'in', 'the', 'middle', 'of', 'a', 'pan', '##de', '##mic', '.', '|', 't', '##wee', '##t', ':', '@', 'pot', '##us', 'bid', '##en', 'blu', '##nder', '##s', '-', '6', 'month', 'update', 'inflation', ',', 'delta', 'mis', '##mana', '##gement', ',', 'co', '##vid', 'for', 'kids', ',', 'abandoning', 'americans', 'in', 'afghanistan', ',', 'arm', '##ing', 'the', 'taliban', ',', 's', '.', 'border', 'crisis', ',', 'breaking', 'job', 'growth', ',', 'abuse', 'of', 'power', '(', 'many', 'ex', '##ec', 'orders', ',', '$', '3', '.', '5', '##t', 'through', 'reconciliation', ',', 'ev', '##iction', 'mora', '##torium', ')', '.', '.', '.', 'what', 'did', 'i', 'miss', '?']\n",
      "Token IDs:  [4861, 1024, 2203, 1997, 23408, 28097, 26821, 24390, 2965, 8817, 1997, 4841, 2071, 4558, 2037, 3847, 1999, 1996, 2690, 1997, 1037, 6090, 3207, 7712, 1012, 1064, 1056, 28394, 2102, 1024, 1030, 8962, 2271, 7226, 2368, 14154, 11563, 2015, 1011, 1020, 3204, 10651, 14200, 1010, 7160, 28616, 24805, 20511, 1010, 2522, 17258, 2005, 4268, 1010, 19816, 4841, 1999, 7041, 1010, 2849, 2075, 1996, 16597, 1010, 1055, 1012, 3675, 5325, 1010, 4911, 3105, 3930, 1010, 6905, 1997, 2373, 1006, 2116, 4654, 8586, 4449, 1010, 1002, 1017, 1012, 1019, 2102, 2083, 16088, 1010, 23408, 28097, 26821, 24390, 1007, 1012, 1012, 1012, 2054, 2106, 1045, 3335, 1029]\n"
     ]
    }
   ],
   "source": [
    "# Print the original sentence.\n",
    "print(' Original: ', sentences[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea3b9c89-44de-4609-a518-3052a589908f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111593/111593 [01:13<00:00, 1526.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For every sentence...\n",
    "for sent in tqdm(sentences):\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca91cb45-1857-4574-a619-8cbba56273d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                       | 0/111593 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\User\\miniconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111593/111593 [01:31<00:00, 1215.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "MAX_SENTENCE_LENGTH = 410\n",
    "\n",
    "# For every sentence...\n",
    "for sent in tqdm(sentences):\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = MAX_SENTENCE_LENGTH,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dba6647-c6d0-4fe5-994b-661ab9b432cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ae78572-5c97-48b2-b8fd-1635b5745830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Statement: End of eviction moratorium means millions of Americans could lose their housing in the middle of a pandemic.| Tweet: BREAKING NEWS: Mitch McConnell accuses President Biden of pushing socialism by implementing the eviction moratorium that will stop millions of Americans from being thrown out on the street this month. RT if you think that Mitch is a heartless idiot!\n",
      "Token IDs: tensor([  101,  4861,  1024,  6174,  6890,  2987,  2102,  2228,  2045,  2323,\n",
      "         2022,  1037,  2976,  6263, 11897,  1012,  1064,  1056, 28394,  2102,\n",
      "         1024,  1030,  2317,  4580,  1998,  2169,  1998,  2296,  2028,  1997,\n",
      "         2068,  6263, 11897, 20929,  1010,  6174,  6890,  2806,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Labels: tensor(0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "index = 10\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[index])\n",
    "print('Token IDs:', input_ids[index])\n",
    "print ('Labels:', labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5374dda6-4c51-4179-a733-e5889f8dd0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([111593, 410])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c66ba4c3-0236-499d-80ca-742b5a2e15a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([111593, 410]), torch.Size([111593, 410]), torch.Size([111593]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape, attention_masks.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f2e335c-d319-42a0-a4f8-719cc21c29bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89,274 training samples\n",
      "22,319 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 80-20 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f06f5391-9385-4566-9200-92eebe31108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 2\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = RandomSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6bc13d4-68d2-481a-8431-21764fed64ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "#model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "402ce2a1-a2bf-43af-86e8-322076484fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08ab44ef-2a35-4b1d-a9a0-83598f033094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\miniconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f73d228-43b8-4a2e-afc4-18df28d17523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "debfe942-a9b0-4990-88d6-50093725dcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "427c284e-eb0e-4e17-a9d0-7f990c56176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46b26116-d8a9-482c-9867-8d1fab53a7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49e6e9bf-6f44-439f-bc7a-97b017247a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = 'Truthseeker2023/checkpoints/checkpoint_with_maxlength_410'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c612aa1f-af4e-47df-88b9-97f64475d915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  44,637.    Elapsed: 0:01:04. Training loss. 0.6881427764892578 Num fake examples 43 Num true examples 37\n",
      "  Batch    80  of  44,637.    Elapsed: 0:02:08. Training loss. 0.7914513349533081 Num fake examples 81 Num true examples 79\n",
      "  Batch   120  of  44,637.    Elapsed: 0:03:12. Training loss. 0.8777908682823181 Num fake examples 125 Num true examples 115\n",
      "  Batch   160  of  44,637.    Elapsed: 0:04:15. Training loss. 0.9716582298278809 Num fake examples 173 Num true examples 147\n",
      "  Batch   200  of  44,637.    Elapsed: 0:05:19. Training loss. 0.10452064871788025 Num fake examples 216 Num true examples 184\n",
      "  Batch   240  of  44,637.    Elapsed: 0:06:25. Training loss. 0.020708760246634483 Num fake examples 253 Num true examples 227\n",
      "  Batch   280  of  44,637.    Elapsed: 0:07:29. Training loss. 0.03479030355811119 Num fake examples 287 Num true examples 273\n",
      "  Batch   320  of  44,637.    Elapsed: 0:08:32. Training loss. 0.11202876269817352 Num fake examples 325 Num true examples 315\n",
      "  Batch   360  of  44,637.    Elapsed: 0:09:36. Training loss. 1.985898494720459 Num fake examples 360 Num true examples 360\n",
      "  Batch   400  of  44,637.    Elapsed: 0:10:40. Training loss. 0.10371658205986023 Num fake examples 407 Num true examples 393\n",
      "  Batch   440  of  44,637.    Elapsed: 0:11:45. Training loss. 0.0271801445633173 Num fake examples 445 Num true examples 435\n",
      "  Batch   480  of  44,637.    Elapsed: 0:12:51. Training loss. 0.08897180110216141 Num fake examples 488 Num true examples 472\n",
      "  Batch   520  of  44,637.    Elapsed: 0:13:59. Training loss. 0.0153731694445014 Num fake examples 522 Num true examples 518\n",
      "  Batch   560  of  44,637.    Elapsed: 0:15:03. Training loss. 0.015581167303025723 Num fake examples 555 Num true examples 565\n",
      "  Batch   600  of  44,637.    Elapsed: 0:16:08. Training loss. 0.011256806552410126 Num fake examples 595 Num true examples 605\n",
      "  Batch   640  of  44,637.    Elapsed: 0:17:15. Training loss. 0.04562423750758171 Num fake examples 634 Num true examples 646\n",
      "  Batch   680  of  44,637.    Elapsed: 0:18:25. Training loss. 0.010194927453994751 Num fake examples 678 Num true examples 682\n",
      "  Batch   720  of  44,637.    Elapsed: 0:19:34. Training loss. 0.016860714182257652 Num fake examples 716 Num true examples 724\n",
      "  Batch   760  of  44,637.    Elapsed: 0:20:44. Training loss. 0.009263450279831886 Num fake examples 757 Num true examples 763\n",
      "  Batch   800  of  44,637.    Elapsed: 0:21:51. Training loss. 0.010023734532296658 Num fake examples 795 Num true examples 805\n",
      "  Batch   840  of  44,637.    Elapsed: 0:22:59. Training loss. 0.009406977333128452 Num fake examples 836 Num true examples 844\n",
      "  Batch   880  of  44,637.    Elapsed: 0:24:07. Training loss. 0.006582132540643215 Num fake examples 874 Num true examples 886\n",
      "  Batch   920  of  44,637.    Elapsed: 0:25:15. Training loss. 1.7512578964233398 Num fake examples 916 Num true examples 924\n",
      "  Batch   960  of  44,637.    Elapsed: 0:26:24. Training loss. 0.008250055834650993 Num fake examples 948 Num true examples 972\n",
      "  Batch 1,000  of  44,637.    Elapsed: 0:27:35. Training loss. 0.01160716637969017 Num fake examples 983 Num true examples 1017\n",
      "  Batch 1,040  of  44,637.    Elapsed: 0:28:44. Training loss. 2.410789728164673 Num fake examples 1026 Num true examples 1054\n",
      "  Batch 1,080  of  44,637.    Elapsed: 0:29:52. Training loss. 0.012737606652081013 Num fake examples 1056 Num true examples 1104\n",
      "  Batch 1,120  of  44,637.    Elapsed: 0:30:59. Training loss. 0.010686011984944344 Num fake examples 1092 Num true examples 1148\n",
      "  Batch 1,160  of  44,637.    Elapsed: 0:32:05. Training loss. 0.0075628990307450294 Num fake examples 1124 Num true examples 1196\n",
      "  Batch 1,200  of  44,637.    Elapsed: 0:33:10. Training loss. 0.009421407245099545 Num fake examples 1165 Num true examples 1235\n",
      "  Batch 1,240  of  44,637.    Elapsed: 0:34:16. Training loss. 0.01551435049623251 Num fake examples 1210 Num true examples 1270\n",
      "  Batch 1,280  of  44,637.    Elapsed: 0:35:21. Training loss. 0.00825757160782814 Num fake examples 1253 Num true examples 1307\n",
      "  Batch 1,320  of  44,637.    Elapsed: 0:36:27. Training loss. 0.009066997095942497 Num fake examples 1290 Num true examples 1350\n",
      "  Batch 1,360  of  44,637.    Elapsed: 0:37:32. Training loss. 0.008912649936974049 Num fake examples 1326 Num true examples 1394\n",
      "  Batch 1,400  of  44,637.    Elapsed: 0:38:39. Training loss. 0.006620254833251238 Num fake examples 1370 Num true examples 1430\n",
      "  Batch 1,440  of  44,637.    Elapsed: 0:39:45. Training loss. 0.0051931822672486305 Num fake examples 1404 Num true examples 1476\n",
      "  Batch 1,480  of  44,637.    Elapsed: 0:40:50. Training loss. 0.0035423878580331802 Num fake examples 1437 Num true examples 1523\n",
      "  Batch 1,520  of  44,637.    Elapsed: 0:41:56. Training loss. 0.010964644141495228 Num fake examples 1482 Num true examples 1558\n",
      "  Batch 1,560  of  44,637.    Elapsed: 0:43:02. Training loss. 0.005062130279839039 Num fake examples 1529 Num true examples 1591\n",
      "  Batch 1,600  of  44,637.    Elapsed: 0:44:08. Training loss. 0.004108453635126352 Num fake examples 1572 Num true examples 1628\n",
      "  Batch 1,640  of  44,637.    Elapsed: 0:45:13. Training loss. 0.005685131531208754 Num fake examples 1610 Num true examples 1670\n",
      "  Batch 1,680  of  44,637.    Elapsed: 0:46:19. Training loss. 0.004786548670381308 Num fake examples 1643 Num true examples 1717\n",
      "  Batch 1,720  of  44,637.    Elapsed: 0:47:26. Training loss. 0.009713063016533852 Num fake examples 1682 Num true examples 1758\n",
      "  Batch 1,760  of  44,637.    Elapsed: 0:48:34. Training loss. 0.011097054928541183 Num fake examples 1721 Num true examples 1799\n",
      "  Batch 1,800  of  44,637.    Elapsed: 0:49:42. Training loss. 0.008678089827299118 Num fake examples 1750 Num true examples 1850\n",
      "  Batch 1,840  of  44,637.    Elapsed: 0:50:49. Training loss. 0.010351244360208511 Num fake examples 1793 Num true examples 1887\n",
      "  Batch 1,880  of  44,637.    Elapsed: 0:51:55. Training loss. 0.006048236973583698 Num fake examples 1829 Num true examples 1931\n",
      "  Batch 1,920  of  44,637.    Elapsed: 0:53:02. Training loss. 0.005155791994184256 Num fake examples 1870 Num true examples 1970\n",
      "  Batch 1,960  of  44,637.    Elapsed: 0:54:10. Training loss. 0.0052099768072366714 Num fake examples 1906 Num true examples 2014\n",
      "  Batch 2,000  of  44,637.    Elapsed: 0:55:17. Training loss. 0.009137804619967937 Num fake examples 1943 Num true examples 2057\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epcoh took: 0:55:18\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.17\n",
      "  Validation Loss: 0.04\n",
      "  Validation took: 0:12:25\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  44,637.    Elapsed: 0:01:05. Training loss. 0.005385167431086302 Num fake examples 40 Num true examples 40\n",
      "  Batch    80  of  44,637.    Elapsed: 0:02:11. Training loss. 2.957704782485962 Num fake examples 82 Num true examples 78\n",
      "  Batch   120  of  44,637.    Elapsed: 0:03:16. Training loss. 2.694284200668335 Num fake examples 127 Num true examples 113\n",
      "  Batch   160  of  44,637.    Elapsed: 0:04:21. Training loss. 0.0037911394611001015 Num fake examples 166 Num true examples 154\n",
      "  Batch   200  of  44,637.    Elapsed: 0:05:26. Training loss. 0.0036260078195482492 Num fake examples 200 Num true examples 200\n",
      "  Batch   240  of  44,637.    Elapsed: 0:06:30. Training loss. 0.003941110800951719 Num fake examples 237 Num true examples 243\n",
      "  Batch   280  of  44,637.    Elapsed: 0:07:34. Training loss. 0.002677570329979062 Num fake examples 275 Num true examples 285\n",
      "  Batch   320  of  44,637.    Elapsed: 0:08:38. Training loss. 0.002948228269815445 Num fake examples 313 Num true examples 327\n",
      "  Batch   360  of  44,637.    Elapsed: 0:09:43. Training loss. 0.002994304522871971 Num fake examples 344 Num true examples 376\n",
      "  Batch   400  of  44,637.    Elapsed: 0:10:47. Training loss. 0.005828294903039932 Num fake examples 382 Num true examples 418\n",
      "  Batch   440  of  44,637.    Elapsed: 0:11:50. Training loss. 0.005868678912520409 Num fake examples 421 Num true examples 459\n",
      "  Batch   480  of  44,637.    Elapsed: 0:12:54. Training loss. 2.4682648181915283 Num fake examples 465 Num true examples 495\n",
      "  Batch   520  of  44,637.    Elapsed: 0:13:59. Training loss. 0.005764508619904518 Num fake examples 498 Num true examples 542\n",
      "  Batch   560  of  44,637.    Elapsed: 0:15:03. Training loss. 0.005845988634973764 Num fake examples 536 Num true examples 584\n",
      "  Batch   600  of  44,637.    Elapsed: 0:16:07. Training loss. 0.003958544228225946 Num fake examples 577 Num true examples 623\n",
      "  Batch   640  of  44,637.    Elapsed: 0:17:11. Training loss. 2.597014904022217 Num fake examples 610 Num true examples 670\n",
      "  Batch   680  of  44,637.    Elapsed: 0:18:15. Training loss. 0.007648708298802376 Num fake examples 657 Num true examples 703\n",
      "  Batch   720  of  44,637.    Elapsed: 0:19:19. Training loss. 0.004829603713005781 Num fake examples 689 Num true examples 751\n",
      "  Batch   760  of  44,637.    Elapsed: 0:20:23. Training loss. 0.00711846724152565 Num fake examples 729 Num true examples 791\n",
      "  Batch   800  of  44,637.    Elapsed: 0:21:27. Training loss. 2.6969175338745117 Num fake examples 764 Num true examples 836\n",
      "  Batch   840  of  44,637.    Elapsed: 0:22:31. Training loss. 0.009110305458307266 Num fake examples 796 Num true examples 884\n",
      "  Batch   880  of  44,637.    Elapsed: 0:23:35. Training loss. 0.006518134381622076 Num fake examples 834 Num true examples 926\n",
      "  Batch   920  of  44,637.    Elapsed: 0:24:39. Training loss. 0.00406306330114603 Num fake examples 874 Num true examples 966\n",
      "  Batch   960  of  44,637.    Elapsed: 0:25:43. Training loss. 0.004476838745176792 Num fake examples 906 Num true examples 1014\n",
      "  Batch 1,000  of  44,637.    Elapsed: 0:26:47. Training loss. 0.0037164799869060516 Num fake examples 943 Num true examples 1057\n",
      "  Batch 1,040  of  44,637.    Elapsed: 0:27:50. Training loss. 0.0026746264193207026 Num fake examples 977 Num true examples 1103\n",
      "  Batch 1,080  of  44,637.    Elapsed: 0:28:54. Training loss. 0.0030650335829705 Num fake examples 1016 Num true examples 1144\n",
      "  Batch 1,120  of  44,637.    Elapsed: 0:29:58. Training loss. 2.9131927490234375 Num fake examples 1052 Num true examples 1188\n",
      "  Batch 1,160  of  44,637.    Elapsed: 0:31:02. Training loss. 0.003556947922334075 Num fake examples 1094 Num true examples 1226\n",
      "  Batch 1,200  of  44,637.    Elapsed: 0:32:06. Training loss. 0.004187715705484152 Num fake examples 1134 Num true examples 1266\n",
      "  Batch 1,240  of  44,637.    Elapsed: 0:33:10. Training loss. 2.750990390777588 Num fake examples 1170 Num true examples 1310\n",
      "  Batch 1,280  of  44,637.    Elapsed: 0:34:14. Training loss. 0.007023665588349104 Num fake examples 1207 Num true examples 1353\n",
      "  Batch 1,320  of  44,637.    Elapsed: 0:35:18. Training loss. 0.0056738583371043205 Num fake examples 1258 Num true examples 1382\n",
      "  Batch 1,360  of  44,637.    Elapsed: 0:36:22. Training loss. 0.004276568070054054 Num fake examples 1292 Num true examples 1428\n",
      "  Batch 1,400  of  44,637.    Elapsed: 0:37:25. Training loss. 0.010320358909666538 Num fake examples 1332 Num true examples 1468\n",
      "  Batch 1,440  of  44,637.    Elapsed: 0:38:29. Training loss. 0.00502088712528348 Num fake examples 1376 Num true examples 1504\n",
      "  Batch 1,480  of  44,637.    Elapsed: 0:39:33. Training loss. 0.007886892184615135 Num fake examples 1424 Num true examples 1536\n",
      "  Batch 1,520  of  44,637.    Elapsed: 0:40:37. Training loss. 2.357712745666504 Num fake examples 1452 Num true examples 1588\n",
      "  Batch 1,560  of  44,637.    Elapsed: 0:41:41. Training loss. 0.0054306890815496445 Num fake examples 1491 Num true examples 1629\n",
      "  Batch 1,600  of  44,637.    Elapsed: 0:42:45. Training loss. 0.0060501075349748135 Num fake examples 1538 Num true examples 1662\n",
      "  Batch 1,640  of  44,637.    Elapsed: 0:43:49. Training loss. 0.009514261037111282 Num fake examples 1577 Num true examples 1703\n",
      "  Batch 1,680  of  44,637.    Elapsed: 0:44:53. Training loss. 0.005792323965579271 Num fake examples 1614 Num true examples 1746\n",
      "  Batch 1,720  of  44,637.    Elapsed: 0:45:57. Training loss. 0.0043510552495718 Num fake examples 1651 Num true examples 1789\n",
      "  Batch 1,760  of  44,637.    Elapsed: 0:47:01. Training loss. 0.004182293079793453 Num fake examples 1682 Num true examples 1838\n",
      "  Batch 1,800  of  44,637.    Elapsed: 0:48:05. Training loss. 0.00458253500983119 Num fake examples 1718 Num true examples 1882\n",
      "  Batch 1,840  of  44,637.    Elapsed: 0:49:10. Training loss. 0.0030706734396517277 Num fake examples 1759 Num true examples 1921\n",
      "  Batch 1,880  of  44,637.    Elapsed: 0:50:14. Training loss. 0.004409804008901119 Num fake examples 1800 Num true examples 1960\n",
      "  Batch 1,920  of  44,637.    Elapsed: 0:51:18. Training loss. 0.003471900476142764 Num fake examples 1836 Num true examples 2004\n",
      "  Batch 1,960  of  44,637.    Elapsed: 0:52:21. Training loss. 5.718353748321533 Num fake examples 1873 Num true examples 2047\n",
      "  Batch 2,000  of  44,637.    Elapsed: 0:53:26. Training loss. 0.0037350612692534924 Num fake examples 1918 Num true examples 2082\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epcoh took: 0:53:27\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.17\n",
      "  Validation Loss: 0.04\n",
      "  Validation took: 0:11:56\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  44,637.    Elapsed: 0:01:04. Training loss. 0.005385714583098888 Num fake examples 42 Num true examples 38\n",
      "  Batch    80  of  44,637.    Elapsed: 0:02:09. Training loss. 0.002631923882290721 Num fake examples 78 Num true examples 82\n",
      "  Batch   120  of  44,637.    Elapsed: 0:03:13. Training loss. 0.005873512476682663 Num fake examples 115 Num true examples 125\n",
      "  Batch   160  of  44,637.    Elapsed: 0:04:17. Training loss. 0.0037866299971938133 Num fake examples 150 Num true examples 170\n",
      "  Batch   200  of  44,637.    Elapsed: 0:05:22. Training loss. 2.7202939987182617 Num fake examples 194 Num true examples 206\n",
      "  Batch   240  of  44,637.    Elapsed: 0:06:26. Training loss. 0.025005046278238297 Num fake examples 235 Num true examples 245\n",
      "  Batch   280  of  44,637.    Elapsed: 0:07:30. Training loss. 0.010545272380113602 Num fake examples 283 Num true examples 277\n",
      "  Batch   320  of  44,637.    Elapsed: 0:08:35. Training loss. 0.005193185526877642 Num fake examples 321 Num true examples 319\n",
      "  Batch   360  of  44,637.    Elapsed: 0:09:39. Training loss. 0.004777088295668364 Num fake examples 358 Num true examples 362\n",
      "  Batch   400  of  44,637.    Elapsed: 0:10:43. Training loss. 0.004399838857352734 Num fake examples 398 Num true examples 402\n",
      "  Batch   440  of  44,637.    Elapsed: 0:11:47. Training loss. 0.005697870627045631 Num fake examples 431 Num true examples 449\n",
      "  Batch   480  of  44,637.    Elapsed: 0:12:52. Training loss. 0.004413251765072346 Num fake examples 464 Num true examples 496\n",
      "  Batch   520  of  44,637.    Elapsed: 0:13:56. Training loss. 0.0064691356383264065 Num fake examples 510 Num true examples 530\n",
      "  Batch   560  of  44,637.    Elapsed: 0:15:04. Training loss. 0.006406817119568586 Num fake examples 546 Num true examples 574\n",
      "  Batch   600  of  44,637.    Elapsed: 0:16:09. Training loss. 0.005633989814668894 Num fake examples 584 Num true examples 616\n",
      "  Batch   640  of  44,637.    Elapsed: 0:17:14. Training loss. 0.005195881240069866 Num fake examples 627 Num true examples 653\n",
      "  Batch   680  of  44,637.    Elapsed: 0:18:20. Training loss. 0.00547429546713829 Num fake examples 661 Num true examples 699\n",
      "  Batch   720  of  44,637.    Elapsed: 0:19:26. Training loss. 0.005801858380436897 Num fake examples 696 Num true examples 744\n",
      "  Batch   760  of  44,637.    Elapsed: 0:20:31. Training loss. 0.003534946357831359 Num fake examples 733 Num true examples 787\n",
      "  Batch   800  of  44,637.    Elapsed: 0:21:35. Training loss. 0.004019947722554207 Num fake examples 768 Num true examples 832\n",
      "  Batch   840  of  44,637.    Elapsed: 0:22:40. Training loss. 0.0039113606326282024 Num fake examples 803 Num true examples 877\n",
      "  Batch   880  of  44,637.    Elapsed: 0:23:45. Training loss. 0.004736633040010929 Num fake examples 844 Num true examples 916\n",
      "  Batch   920  of  44,637.    Elapsed: 0:24:50. Training loss. 0.005816927645355463 Num fake examples 885 Num true examples 955\n",
      "  Batch   960  of  44,637.    Elapsed: 0:25:55. Training loss. 0.006199438590556383 Num fake examples 915 Num true examples 1005\n",
      "  Batch 1,000  of  44,637.    Elapsed: 0:26:59. Training loss. 2.4552204608917236 Num fake examples 955 Num true examples 1045\n",
      "  Batch 1,040  of  44,637.    Elapsed: 0:28:03. Training loss. 0.012857056222856045 Num fake examples 996 Num true examples 1084\n",
      "  Batch 1,080  of  44,637.    Elapsed: 0:29:08. Training loss. 2.5325162410736084 Num fake examples 1032 Num true examples 1128\n",
      "  Batch 1,120  of  44,637.    Elapsed: 0:30:12. Training loss. 0.004625127650797367 Num fake examples 1076 Num true examples 1164\n",
      "  Batch 1,160  of  44,637.    Elapsed: 0:31:16. Training loss. 0.004395416006445885 Num fake examples 1109 Num true examples 1211\n",
      "  Batch 1,200  of  44,637.    Elapsed: 0:32:21. Training loss. 0.0036351068411022425 Num fake examples 1156 Num true examples 1244\n",
      "  Batch 1,240  of  44,637.    Elapsed: 0:33:25. Training loss. 0.004515665117651224 Num fake examples 1192 Num true examples 1288\n",
      "  Batch 1,280  of  44,637.    Elapsed: 0:34:30. Training loss. 0.004487474448978901 Num fake examples 1233 Num true examples 1327\n",
      "  Batch 1,320  of  44,637.    Elapsed: 0:35:34. Training loss. 0.0056070671416819096 Num fake examples 1274 Num true examples 1366\n",
      "  Batch 1,360  of  44,637.    Elapsed: 0:36:38. Training loss. 2.6199545860290527 Num fake examples 1311 Num true examples 1409\n",
      "  Batch 1,400  of  44,637.    Elapsed: 0:37:42. Training loss. 0.007798337377607822 Num fake examples 1357 Num true examples 1443\n",
      "  Batch 1,440  of  44,637.    Elapsed: 0:38:47. Training loss. 0.009260106831789017 Num fake examples 1398 Num true examples 1482\n",
      "  Batch 1,480  of  44,637.    Elapsed: 0:39:51. Training loss. 0.007573323789983988 Num fake examples 1438 Num true examples 1522\n",
      "  Batch 1,520  of  44,637.    Elapsed: 0:40:55. Training loss. 0.013850908726453781 Num fake examples 1475 Num true examples 1565\n",
      "  Batch 1,560  of  44,637.    Elapsed: 0:41:59. Training loss. 0.00914396159350872 Num fake examples 1511 Num true examples 1609\n",
      "  Batch 1,600  of  44,637.    Elapsed: 0:43:03. Training loss. 0.014124805107712746 Num fake examples 1554 Num true examples 1646\n",
      "  Batch 1,640  of  44,637.    Elapsed: 0:44:08. Training loss. 0.006608073599636555 Num fake examples 1601 Num true examples 1679\n",
      "  Batch 1,680  of  44,637.    Elapsed: 0:45:12. Training loss. 0.00623359065502882 Num fake examples 1629 Num true examples 1731\n",
      "  Batch 1,720  of  44,637.    Elapsed: 0:46:16. Training loss. 0.006248580291867256 Num fake examples 1664 Num true examples 1776\n",
      "  Batch 1,760  of  44,637.    Elapsed: 0:47:20. Training loss. 0.006017306819558144 Num fake examples 1710 Num true examples 1810\n",
      "  Batch 1,800  of  44,637.    Elapsed: 0:48:25. Training loss. 0.004244043491780758 Num fake examples 1745 Num true examples 1855\n",
      "  Batch 1,840  of  44,637.    Elapsed: 0:49:29. Training loss. 0.003939100541174412 Num fake examples 1777 Num true examples 1903\n",
      "  Batch 1,880  of  44,637.    Elapsed: 0:50:33. Training loss. 0.005389408674091101 Num fake examples 1812 Num true examples 1948\n",
      "  Batch 1,920  of  44,637.    Elapsed: 0:51:37. Training loss. 0.012144971638917923 Num fake examples 1857 Num true examples 1983\n",
      "  Batch 1,960  of  44,637.    Elapsed: 0:52:41. Training loss. 0.006362941116094589 Num fake examples 1895 Num true examples 2025\n",
      "  Batch 2,000  of  44,637.    Elapsed: 0:53:46. Training loss. 0.00511701125651598 Num fake examples 1942 Num true examples 2058\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epcoh took: 0:53:47\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.17\n",
      "  Validation Loss: 0.04\n",
      "  Validation took: 0:12:10\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  44,637.    Elapsed: 0:01:04. Training loss. 0.006576165556907654 Num fake examples 40 Num true examples 40\n",
      "  Batch    80  of  44,637.    Elapsed: 0:02:08. Training loss. 0.005596271716058254 Num fake examples 79 Num true examples 81\n",
      "  Batch   120  of  44,637.    Elapsed: 0:03:12. Training loss. 0.005017479881644249 Num fake examples 116 Num true examples 124\n",
      "  Batch   160  of  44,637.    Elapsed: 0:04:16. Training loss. 0.005504557862877846 Num fake examples 158 Num true examples 162\n",
      "  Batch   200  of  44,637.    Elapsed: 0:05:20. Training loss. 0.005928384605795145 Num fake examples 192 Num true examples 208\n",
      "  Batch   240  of  44,637.    Elapsed: 0:06:24. Training loss. 0.008103352040052414 Num fake examples 239 Num true examples 241\n",
      "  Batch   280  of  44,637.    Elapsed: 0:07:28. Training loss. 2.7059807777404785 Num fake examples 273 Num true examples 287\n",
      "  Batch   320  of  44,637.    Elapsed: 0:08:32. Training loss. 0.005639783572405577 Num fake examples 307 Num true examples 333\n",
      "  Batch   360  of  44,637.    Elapsed: 0:09:36. Training loss. 0.005477090831845999 Num fake examples 352 Num true examples 368\n",
      "  Batch   400  of  44,637.    Elapsed: 0:10:40. Training loss. 0.004123262129724026 Num fake examples 394 Num true examples 406\n",
      "  Batch   440  of  44,637.    Elapsed: 0:11:44. Training loss. 0.004405410960316658 Num fake examples 432 Num true examples 448\n",
      "  Batch   480  of  44,637.    Elapsed: 0:12:48. Training loss. 2.6700334548950195 Num fake examples 476 Num true examples 484\n",
      "  Batch   520  of  44,637.    Elapsed: 0:13:52. Training loss. 0.009759980253875256 Num fake examples 515 Num true examples 525\n",
      "  Batch   560  of  44,637.    Elapsed: 0:14:56. Training loss. 0.009024931117892265 Num fake examples 561 Num true examples 559\n",
      "  Batch   600  of  44,637.    Elapsed: 0:16:00. Training loss. 0.004988112486898899 Num fake examples 601 Num true examples 599\n",
      "  Batch   640  of  44,637.    Elapsed: 0:17:04. Training loss. 0.004564222879707813 Num fake examples 639 Num true examples 641\n",
      "  Batch   680  of  44,637.    Elapsed: 0:18:08. Training loss. 0.00774681381881237 Num fake examples 673 Num true examples 687\n",
      "  Batch   720  of  44,637.    Elapsed: 0:19:12. Training loss. 0.005255670286715031 Num fake examples 719 Num true examples 721\n",
      "  Batch   760  of  44,637.    Elapsed: 0:20:16. Training loss. 0.00497021060436964 Num fake examples 760 Num true examples 760\n",
      "  Batch   800  of  44,637.    Elapsed: 0:21:20. Training loss. 0.005377345718443394 Num fake examples 798 Num true examples 802\n",
      "  Batch   840  of  44,637.    Elapsed: 0:22:24. Training loss. 2.6584267616271973 Num fake examples 840 Num true examples 840\n",
      "  Batch   880  of  44,637.    Elapsed: 0:23:28. Training loss. 0.006645566783845425 Num fake examples 873 Num true examples 887\n",
      "  Batch   920  of  44,637.    Elapsed: 0:24:32. Training loss. 0.0070573980920016766 Num fake examples 915 Num true examples 925\n",
      "  Batch   960  of  44,637.    Elapsed: 0:25:36. Training loss. 0.0044244457967579365 Num fake examples 964 Num true examples 956\n",
      "  Batch 1,000  of  44,637.    Elapsed: 0:26:40. Training loss. 0.004479982890188694 Num fake examples 1004 Num true examples 996\n",
      "  Batch 1,040  of  44,637.    Elapsed: 0:27:44. Training loss. 0.005361397750675678 Num fake examples 1044 Num true examples 1036\n",
      "  Batch 1,080  of  44,637.    Elapsed: 0:28:48. Training loss. 0.007991252467036247 Num fake examples 1088 Num true examples 1072\n",
      "  Batch 1,120  of  44,637.    Elapsed: 0:29:52. Training loss. 2.395636558532715 Num fake examples 1125 Num true examples 1115\n",
      "  Batch 1,160  of  44,637.    Elapsed: 0:30:56. Training loss. 0.006059644743800163 Num fake examples 1174 Num true examples 1146\n",
      "  Batch 1,200  of  44,637.    Elapsed: 0:32:00. Training loss. 0.007708971854299307 Num fake examples 1211 Num true examples 1189\n",
      "  Batch 1,240  of  44,637.    Elapsed: 0:33:04. Training loss. 0.005806836765259504 Num fake examples 1244 Num true examples 1236\n",
      "  Batch 1,280  of  44,637.    Elapsed: 0:34:07. Training loss. 0.0061173089779913425 Num fake examples 1288 Num true examples 1272\n",
      "  Batch 1,320  of  44,637.    Elapsed: 0:35:11. Training loss. 0.005552978720515966 Num fake examples 1322 Num true examples 1318\n",
      "  Batch 1,360  of  44,637.    Elapsed: 0:36:15. Training loss. 0.0051271263509988785 Num fake examples 1358 Num true examples 1362\n",
      "  Batch 1,400  of  44,637.    Elapsed: 0:37:19. Training loss. 0.004131580237299204 Num fake examples 1407 Num true examples 1393\n",
      "  Batch 1,440  of  44,637.    Elapsed: 0:38:23. Training loss. 0.004039758816361427 Num fake examples 1442 Num true examples 1438\n",
      "  Batch 1,480  of  44,637.    Elapsed: 0:39:27. Training loss. 0.0037135968450456858 Num fake examples 1474 Num true examples 1486\n",
      "  Batch 1,520  of  44,637.    Elapsed: 0:40:31. Training loss. 0.0042220246978104115 Num fake examples 1516 Num true examples 1524\n",
      "  Batch 1,560  of  44,637.    Elapsed: 0:41:35. Training loss. 0.005117559339851141 Num fake examples 1560 Num true examples 1560\n",
      "  Batch 1,600  of  44,637.    Elapsed: 0:42:39. Training loss. 0.00373749528080225 Num fake examples 1594 Num true examples 1606\n",
      "  Batch 1,640  of  44,637.    Elapsed: 0:43:43. Training loss. 0.0030186246149241924 Num fake examples 1625 Num true examples 1655\n",
      "  Batch 1,680  of  44,637.    Elapsed: 0:44:47. Training loss. 0.003488924354314804 Num fake examples 1659 Num true examples 1701\n",
      "  Batch 1,720  of  44,637.    Elapsed: 0:45:50. Training loss. 0.0035779783502221107 Num fake examples 1702 Num true examples 1738\n",
      "  Batch 1,760  of  44,637.    Elapsed: 0:46:54. Training loss. 0.0035995268262922764 Num fake examples 1738 Num true examples 1782\n",
      "  Batch 1,800  of  44,637.    Elapsed: 0:47:58. Training loss. 0.002713882364332676 Num fake examples 1773 Num true examples 1827\n",
      "  Batch 1,840  of  44,637.    Elapsed: 0:49:02. Training loss. 0.003338861744850874 Num fake examples 1814 Num true examples 1866\n",
      "  Batch 1,880  of  44,637.    Elapsed: 0:50:06. Training loss. 0.0027175480499863625 Num fake examples 1857 Num true examples 1903\n",
      "  Batch 1,920  of  44,637.    Elapsed: 0:51:10. Training loss. 0.002829635050147772 Num fake examples 1892 Num true examples 1948\n",
      "  Batch 1,960  of  44,637.    Elapsed: 0:52:14. Training loss. 0.0067815026268363 Num fake examples 1933 Num true examples 1987\n",
      "  Batch 2,000  of  44,637.    Elapsed: 0:53:18. Training loss. 0.003194025019183755 Num fake examples 1972 Num true examples 2028\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epcoh took: 0:53:19\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.17\n",
      "  Validation Loss: 0.04\n",
      "  Validation took: 0:11:56\n",
      "\n",
      "Training complete!\n",
      "Total training took 4:24:26 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_fake_examples = 0\n",
    "    total_true_examples = 0\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step > 2000:\n",
    "            break\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}. Training loss. {:} Num fake examples {:} Num true examples {:}'.format(step, len(train_dataloader), elapsed, train_loss,total_fake_examples, total_true_examples ))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(torch.int64).to(device)\n",
    "        total_fake_examples += (b_labels == 1).sum().item()\n",
    "        total_true_examples += (b_labels == 0).sum().item()\n",
    "        #print (f\"{b_labels.shape=}\")\n",
    "        b_labels_one_hot = torch.nn.functional.one_hot(b_labels, num_classes=2).float()\n",
    "        #print (b_input_ids.shape, b_labels.shape, b_input_mask.shape, b_labels_one_hot.shape, b_labels_one_hot.dtype)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # It returns different numbers of parameters depending on what arguments\n",
    "        # arge given and what flags are set. For our useage here, it returns\n",
    "        # the loss (because we provided labels) and the \"logits\"--the model\n",
    "        # outputs prior to activation.\n",
    "        output = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels_one_hot)\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        train_loss= loss.item()\n",
    "        total_train_loss += train_loss\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "        #print (f\"Training loss\", loss.item())\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    \n",
    "    for step, batch in enumerate(validation_dataloader):\n",
    "        if step > 2000:\n",
    "            break\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(torch.int64).to(device)\n",
    "        b_labels_one_hot = torch.nn.functional.one_hot(b_labels, num_classes=2).float()\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            \n",
    "            output = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels_one_hot)\n",
    "            loss = output.loss\n",
    "            logits = output.logits\n",
    "\n",
    "\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "    #Save model checkpoint\n",
    "    model.save_pretrained(SAVE_DIR)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe6bb68-246e-4093-afb2-cf4363067b8a",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "873821bc-1f66-44fe-acd0-7a312c358621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 410])\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 5.7656, -5.7749]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "def encode(sentence):\n",
    "    return tokenizer.encode_plus(\n",
    "                        sentence,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 410,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "\n",
    "SENTENCE_INDEX = 5000\n",
    "encoded_dict = encode(sentences[SENTENCE_INDEX])\n",
    "input_id = encoded_dict['input_ids']\n",
    "attention_mask = encoded_dict['attention_mask']\n",
    "print (input_id.shape)\n",
    "model.eval()\n",
    "output = model(\n",
    "            input_id,\n",
    "            #input_id.cuda(),\n",
    "            token_type_ids=None, \n",
    "            attention_mask=attention_mask, return_dict=True)\n",
    "            #attention_mask=attention_mask.cuda(), return_dict=True)\n",
    "print (output)\n",
    "print (labels[SENTENCE_INDEX])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fdd7d7-aae7-4b04-afcc-664161c3e215",
   "metadata": {},
   "source": [
    "## Using validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7064487-72c4-43ba-a7ac-128221794554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a534156a-198b-495a-94b8-20b3e28f10a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 410])\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6341, -2.7243]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SENTENCE_INDEX = 5000\n",
    "model.eval()\n",
    "print (torch.unsqueeze(val_dataset[SENTENCE_INDEX][0], dim=0).shape)\n",
    "#print (torch.unsqueeze(val_dataset[SENTENCE_INDEX][0].cuda(), dim=0).shape)\n",
    "#output = model(torch.unsqueeze(val_dataset[SENTENCE_INDEX][0].cuda(), dim=0),\n",
    "output = model(torch.unsqueeze(val_dataset[SENTENCE_INDEX][0], dim=0),\n",
    "            token_type_ids=None, \n",
    "            attention_mask=torch.unsqueeze(val_dataset[SENTENCE_INDEX][0],dim=0), return_dict=True)\n",
    "            #attention_mask=torch.unsqueeze(val_dataset[SENTENCE_INDEX][0].cuda(),dim=0), return_dict=True)\n",
    "print (output)\n",
    "print (labels[SENTENCE_INDEX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca1659b2-4081-4161-b331-c0092badb90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.4170,  5.4192],\n",
      "        [-4.4809,  4.4294]]) tensor([1, 1])\n",
      "tensor([[ 5.7742, -5.7855],\n",
      "        [ 5.7746, -5.7806]]) tensor([0, 0])\n",
      "tensor([[ 5.7438, -5.7422],\n",
      "        [ 5.7626, -5.7626]]) tensor([0, 0])\n",
      "tensor([[-5.4100,  5.4047],\n",
      "        [ 5.7720, -5.7827]]) tensor([1, 0])\n",
      "tensor([[ 5.7736, -5.7852],\n",
      "        [-5.4027,  5.3998]]) tensor([0, 1])\n",
      "tensor([[ 5.7738, -5.7848],\n",
      "        [ 5.6325, -5.6466]]) tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(validation_dataloader):\n",
    "    if step > 5:\n",
    "        break\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(torch.int64).to(device)\n",
    "    b_labels_one_hot = torch.nn.functional.one_hot(b_labels, num_classes=2).float()\n",
    "    \n",
    "    # Tell pytorch not to bother with constructing the compute graph during\n",
    "    # the forward pass, since this is only needed for backprop (training).\n",
    "    with torch.no_grad():        \n",
    "\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        # token_type_ids is the same as the \"segment ids\", which \n",
    "        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        \n",
    "        output = model(b_input_ids, \n",
    "                               token_type_ids=None, \n",
    "                               attention_mask=b_input_mask,\n",
    "                               labels=b_labels_one_hot)\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "        print (logits, b_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f170c-91e6-4153-a3cf-872a6550bf69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
