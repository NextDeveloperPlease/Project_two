{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52b56a26-dc6a-43bb-8398-a76752038969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 134,198\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>statement</th>\n",
       "      <th>target</th>\n",
       "      <th>BinaryNumTarget</th>\n",
       "      <th>manual_keywords</th>\n",
       "      <th>tweet</th>\n",
       "      <th>5_label_majority_answer</th>\n",
       "      <th>3_label_majority_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60376</th>\n",
       "      <td>60376</td>\n",
       "      <td>W. Gardner</td>\n",
       "      <td>\"The majority of Austinites rent\" the places t...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>austinites, rent</td>\n",
       "      <td>Hello Austinites! I am looking for someone to ...</td>\n",
       "      <td>Mostly Agree</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40041</th>\n",
       "      <td>40041</td>\n",
       "      <td>Louis Jacobson</td>\n",
       "      <td>\"There are more words in the IRS code than the...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>More words, IRS code, Bible</td>\n",
       "      <td>\"There are more words in the IRS code than the...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67935</th>\n",
       "      <td>67935</td>\n",
       "      <td>Katie Sanders</td>\n",
       "      <td>The Keystone pipeline creates \"35 permanent jo...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>keystone pipeline, 35 jobs</td>\n",
       "      <td>@HouseGOP @RepMTG Garbage lies. 35 permanent j...</td>\n",
       "      <td>Mostly Agree</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49984</th>\n",
       "      <td>49984</td>\n",
       "      <td>Bill McCarthy</td>\n",
       "      <td>The mRNA COVID-19 vaccines are really gene the...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>vaccine,gene therapy,mRNA</td>\n",
       "      <td>@WolfieSmiff another unreported vaccine injury...</td>\n",
       "      <td>Mostly Agree</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116772</th>\n",
       "      <td>116772</td>\n",
       "      <td>Miriam Valverde</td>\n",
       "      <td>Says Joe Biden and Kamala Harris support abort...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Biden, Harris, support abortion</td>\n",
       "      <td>@JoeBiden @KamalaHarris How sad this would be ...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65566</th>\n",
       "      <td>65566</td>\n",
       "      <td>Jon Greenberg</td>\n",
       "      <td>\"The majority support (raising) the minimum wa...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>majority, support, minimum wage</td>\n",
       "      <td>@POTUS Yeah, but a majority support universal ...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99271</th>\n",
       "      <td>99271</td>\n",
       "      <td>Tom Kertscher</td>\n",
       "      <td>Says Donald Trump offered his hotel to our tro...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Trump, offered, hotel, troops</td>\n",
       "      <td>@unscriptedmike If Trump really cared that muc...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48764</th>\n",
       "      <td>48764</td>\n",
       "      <td>Tom Kertscher</td>\n",
       "      <td>\"The top 1% pays 90% of income taxes.</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>top 1%, 90% income tax</td>\n",
       "      <td>This is your daily reminder that the Top 1% of...</td>\n",
       "      <td>Mostly Agree</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108392</th>\n",
       "      <td>108392</td>\n",
       "      <td>Daniel Funke</td>\n",
       "      <td>A judge has ordered a \"HAND RECOUNT by an INDE...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>judge, recount, Michigan</td>\n",
       "      <td>@OfficialKYChick @HannahDrake628 @politico i h...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34207</th>\n",
       "      <td>34207</td>\n",
       "      <td>Alan Gathright</td>\n",
       "      <td>Says Donald Trump mocked \"someone with a disab...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Donald Trump, mocked, disability</td>\n",
       "      <td>Finally saw the John Dingell clip from Donald ...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134198 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0           author  \\\n",
       "60376        60376       W. Gardner   \n",
       "40041        40041   Louis Jacobson   \n",
       "67935        67935    Katie Sanders   \n",
       "49984        49984    Bill McCarthy   \n",
       "116772      116772  Miriam Valverde   \n",
       "...            ...              ...   \n",
       "65566        65566    Jon Greenberg   \n",
       "99271        99271    Tom Kertscher   \n",
       "48764        48764    Tom Kertscher   \n",
       "108392      108392     Daniel Funke   \n",
       "34207        34207   Alan Gathright   \n",
       "\n",
       "                                                statement  target  \\\n",
       "60376   \"The majority of Austinites rent\" the places t...    True   \n",
       "40041   \"There are more words in the IRS code than the...    True   \n",
       "67935   The Keystone pipeline creates \"35 permanent jo...    True   \n",
       "49984   The mRNA COVID-19 vaccines are really gene the...   False   \n",
       "116772  Says Joe Biden and Kamala Harris support abort...   False   \n",
       "...                                                   ...     ...   \n",
       "65566   \"The majority support (raising) the minimum wa...    True   \n",
       "99271   Says Donald Trump offered his hotel to our tro...   False   \n",
       "48764               \"The top 1% pays 90% of income taxes.   False   \n",
       "108392  A judge has ordered a \"HAND RECOUNT by an INDE...   False   \n",
       "34207   Says Donald Trump mocked \"someone with a disab...    True   \n",
       "\n",
       "        BinaryNumTarget                   manual_keywords  \\\n",
       "60376               1.0                  austinites, rent   \n",
       "40041               1.0       More words, IRS code, Bible   \n",
       "67935               1.0        keystone pipeline, 35 jobs   \n",
       "49984               0.0         vaccine,gene therapy,mRNA   \n",
       "116772              0.0   Biden, Harris, support abortion   \n",
       "...                 ...                               ...   \n",
       "65566               1.0   majority, support, minimum wage   \n",
       "99271               0.0     Trump, offered, hotel, troops   \n",
       "48764               0.0            top 1%, 90% income tax   \n",
       "108392              0.0          judge, recount, Michigan   \n",
       "34207               1.0  Donald Trump, mocked, disability   \n",
       "\n",
       "                                                    tweet  \\\n",
       "60376   Hello Austinites! I am looking for someone to ...   \n",
       "40041   \"There are more words in the IRS code than the...   \n",
       "67935   @HouseGOP @RepMTG Garbage lies. 35 permanent j...   \n",
       "49984   @WolfieSmiff another unreported vaccine injury...   \n",
       "116772  @JoeBiden @KamalaHarris How sad this would be ...   \n",
       "...                                                   ...   \n",
       "65566   @POTUS Yeah, but a majority support universal ...   \n",
       "99271   @unscriptedmike If Trump really cared that muc...   \n",
       "48764   This is your daily reminder that the Top 1% of...   \n",
       "108392  @OfficialKYChick @HannahDrake628 @politico i h...   \n",
       "34207   Finally saw the John Dingell clip from Donald ...   \n",
       "\n",
       "       5_label_majority_answer 3_label_majority_answer  \n",
       "60376             Mostly Agree                   Agree  \n",
       "40041                    Agree                   Agree  \n",
       "67935             Mostly Agree                   Agree  \n",
       "49984             Mostly Agree                   Agree  \n",
       "116772                   Agree                   Agree  \n",
       "...                        ...                     ...  \n",
       "65566                    Agree                   Agree  \n",
       "99271                    Agree                   Agree  \n",
       "48764             Mostly Agree                   Agree  \n",
       "108392                   Agree                   Agree  \n",
       "34207                    Agree                   Agree  \n",
       "\n",
       "[134198 rows x 9 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATASET_PATH = \"Truth_Seeker_Model_Dataset.csv\"\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "df = df.sample(frac=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d62b6ab2-3ef8-4e2f-97bb-ef004a8ebbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df, statement_column='statement', test_size=0.2, random_state=None):\n",
    "    # Step 1: Get unique statements\n",
    "    unique_statements = df[statement_column].drop_duplicates()\n",
    "    \n",
    "    # Step 2: Perform an 80/20 split on the unique statements\n",
    "    train_statements, test_statements = train_test_split(\n",
    "        unique_statements, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Step 3: Filter the original dataframe using the split statements\n",
    "    train_df = df[df[statement_column].isin(train_statements)].reset_index(drop=True)\n",
    "    test_df = df[df[statement_column].isin(test_statements)].reset_index(drop=True)\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8da093e7-2473-4402-963e-7205d4adac55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111593, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(df[df['5_label_majority_answer'] == 'NO MAJORITY'].index, axis=0)\n",
    "#df = df.drop(df[df['3_label_majority_answer'] == 'NO MAJORITY'].index, axis=0)\n",
    "\n",
    "#df = df.drop('5_label_majority_answer', axis=1)\n",
    "df = df.drop('3_label_majority_answer', axis=1)\n",
    "\n",
    "train,test = split_dataframe(df, random_state=42)\n",
    "\n",
    "\n",
    "#labels = df[\"BinaryNumTarget\"].values\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59cffe6a-71d1-425e-9f78-a089d4e5e7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences = 'Statement: ' + df['statement'] + '| Tweet: ' +df['tweet']\n",
    "#sentences = df['target'].astype(str) + 'Statement: ' + df['statement'] + '| Tweet: ' + df['tweet']\n",
    "sentences = train['target'].astype(str) + ' Statement: ' + train['statement'] + '| Tweet: ' + train['tweet']\n",
    "sentences_test = test['target'].astype(str) + ' Statement: ' + test['statement'] + '| Tweet: ' + test['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13803549-d9c5-4d3f-8d3c-c496cba66ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'True Statement: \"The majority of Austinites rent\" the places they live.| Tweet: Hello Austinites! I am looking for someone to go on camera for a story. Anyone unable to afford Austin\\'s rising rent prices and thinking about moving out of town/state? @SpectrumNews1TX #austin \\n\\nIf you\\'re interested in being interviewed, email me at jamil.donith@charter.com'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15da247e-f4b0-41a8-bdb7-b7e04c3ddb50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'True Statement: The Keystone pipeline creates \"35 permanent jobs\" after construction, according to a government report.| Tweet: @HouseGOP @RepMTG Garbage lies. 35 permanent jobs from the Keystone Pipeline lol. 35.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a7ec289-2593-4715-8124-6fb1179de6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 111593 entries, 60376 to 34207\n",
      "Data columns (total 8 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   Unnamed: 0               111593 non-null  int64  \n",
      " 1   author                   111593 non-null  object \n",
      " 2   statement                111593 non-null  object \n",
      " 3   target                   111593 non-null  bool   \n",
      " 4   BinaryNumTarget          111593 non-null  float64\n",
      " 5   manual_keywords          111593 non-null  object \n",
      " 6   tweet                    111593 non-null  object \n",
      " 7   5_label_majority_answer  111593 non-null  object \n",
      "dtypes: bool(1), float64(1), int64(1), object(5)\n",
      "memory usage: 6.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53567947-0e0f-4120-8a9b-92c070809b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_truthfulness_4way(row):\n",
    "    if row['target'] == True:\n",
    "        if row['5_label_majority_answer'] == 'Agree':\n",
    "            return 'True'\n",
    "        elif row['5_label_majority_answer'] == 'Disagree':\n",
    "            return 'False'\n",
    "        elif row['5_label_majority_answer'] == 'Mostly Agree':\n",
    "            return 'True'\n",
    "        elif row['5_label_majority_answer'] == 'Mostly Disagree':\n",
    "            return 'False'\n",
    "    else:\n",
    "        if row['5_label_majority_answer'] == 'Agree':\n",
    "            return 'False'\n",
    "        elif row['5_label_majority_answer'] == 'Disagree':\n",
    "            return 'True'\n",
    "        elif row['5_label_majority_answer'] == 'Mostly Agree':\n",
    "            return 'False'\n",
    "        elif row['5_label_majority_answer'] == 'Mostly Disagree':\n",
    "            return 'True'\n",
    "    return None\n",
    "\n",
    "def generate_truthfulness_2way(row):\n",
    "    if row['target'] == True:\n",
    "        if row['3_label_majority_answer'] == 'Agree':\n",
    "            return \"True\"\n",
    "        elif row['3_label_majority_answer'] == 'Disagree':\n",
    "            return \"False\"\n",
    "    else:\n",
    "        if row['3_label_majority_answer'] == 'Agree':\n",
    "            return \"False\"\n",
    "        elif row['3_label_majority_answer'] == 'Disagree':\n",
    "            return \"True\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6af7d7a7-da69-45f3-a5c2-054e4e63ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame()\n",
    "df2['4-way-label'] = train.apply(lambda x: generate_truthfulness_4way(x), axis=1)\n",
    "#df2['2-way-label'] = train.apply(lambda x: generate_truthfulness_2way(x), axis=1)\n",
    "\n",
    "df2_test = pd.DataFrame()\n",
    "df2_test['4-way-label'] = test.apply(lambda x: generate_truthfulness_4way(x), axis=1)\n",
    "#df2_test['2-way-label'] = test.apply(lambda x: generate_truthfulness_2way(x), axis=1)\n",
    "\n",
    "#df2.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af0b815c-bc6b-4e8a-bbfc-74034db88b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((87305, 1), (24288, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape, df2_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24f91527-667e-4278-8f96-d58d438f7e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_220983/3150518886.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df2['4-way-label'] = df2['4-way-label'].replace({'True': 0, 'False': 1})\n",
      "/tmp/ipykernel_220983/3150518886.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df2_test['4-way-label'] = df2_test['4-way-label'].replace({'True': 0, 'False': 1})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((87305,), (24288,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df2['2-way-label'] = df2['2-way-label'].replace({'True': 0, 'False': 1})\n",
    "df2['4-way-label'] = df2['4-way-label'].replace({'True': 0, 'False': 1})\n",
    "#labels = df2[\"2-way-label\"].values\n",
    "labels = df2['4-way-label'].values\n",
    "\n",
    "#df2_test['2-way-label'] = df2_test['2-way-label'].replace({'True': 0, 'False': 1})\n",
    "df2_test['4-way-label'] = df2_test['4-way-label'].replace({'True': 0, 'False': 1})\n",
    "#labels_test = df2_test[\"2-way-label\"].values\n",
    "labels_test = df2_test['4-way-label'].values\n",
    "\n",
    "labels.shape, labels_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5affeb06-8cd1-4b58-9000-09419efac91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(       4-way-label\n",
       " 0                0\n",
       " 1                0\n",
       " 2                1\n",
       " 3                0\n",
       " 4                0\n",
       " ...            ...\n",
       " 87300            0\n",
       " 87301            1\n",
       " 87302            1\n",
       " 87303            1\n",
       " 87304            0\n",
       " \n",
       " [87305 rows x 1 columns],\n",
       "        4-way-label\n",
       " 0                0\n",
       " 1                1\n",
       " 2                0\n",
       " 3                0\n",
       " 4                0\n",
       " ...            ...\n",
       " 24283            1\n",
       " 24284            0\n",
       " 24285            0\n",
       " 24286            1\n",
       " 24287            0\n",
       " \n",
       " [24288 rows x 1 columns])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2, df2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15df6ee4-c79f-478a-b893-f5c29195ed7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       4-way-label\n",
      "0                0\n",
      "1                0\n",
      "2                1\n",
      "3                0\n",
      "4                0\n",
      "...            ...\n",
      "87300            0\n",
      "87301            1\n",
      "87302            1\n",
      "87303            1\n",
      "87304            0\n",
      "\n",
      "[87305 rows x 1 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#df2['2-way-label'].plot(kind='hist')\n",
    "df2['4-way-label'].plot(kind='hist')\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cefbc76-0e2c-4afc-bc90-c1cd2c172cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((87305,), (87305, 1), (24288,), (24288, 1))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#labels = df2['2-way-label'].values\n",
    "labels.shape, df2.shape, labels_test.shape, df2_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01944a71-51db-47e2-b7f8-41fe07d1609b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5031cf4e-3047-4bdd-99d7-90d1f2fb2d87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  True Statement: \"The majority of Austinites rent\" the places they live.| Tweet: Hello Austinites! I am looking for someone to go on camera for a story. Anyone unable to afford Austin's rising rent prices and thinking about moving out of town/state? @SpectrumNews1TX #austin \n",
      "\n",
      "If you're interested in being interviewed, email me at jamil.donith@charter.com\n",
      "Tokenized:  ['true', 'statement', ':', '\"', 'the', 'majority', 'of', 'austin', '##ites', 'rent', '\"', 'the', 'places', 'they', 'live', '.', '|', 't', '##wee', '##t', ':', 'hello', 'austin', '##ites', '!', 'i', 'am', 'looking', 'for', 'someone', 'to', 'go', 'on', 'camera', 'for', 'a', 'story', '.', 'anyone', 'unable', 'to', 'afford', 'austin', \"'\", 's', 'rising', 'rent', 'prices', 'and', 'thinking', 'about', 'moving', 'out', 'of', 'town', '/', 'state', '?', '@', 'spectrum', '##ne', '##ws', '##1', '##t', '##x', '#', 'austin', 'if', 'you', \"'\", 're', 'interested', 'in', 'being', 'interviewed', ',', 'email', 'me', 'at', 'jam', '##il', '.', 'don', '##ith', '@', 'charter', '.', 'com']\n",
      "Token IDs:  [2995, 4861, 1024, 1000, 1996, 3484, 1997, 5899, 7616, 9278, 1000, 1996, 3182, 2027, 2444, 1012, 1064, 1056, 28394, 2102, 1024, 7592, 5899, 7616, 999, 1045, 2572, 2559, 2005, 2619, 2000, 2175, 2006, 4950, 2005, 1037, 2466, 1012, 3087, 4039, 2000, 8984, 5899, 1005, 1055, 4803, 9278, 7597, 1998, 3241, 2055, 3048, 2041, 1997, 2237, 1013, 2110, 1029, 1030, 8674, 2638, 9333, 2487, 2102, 2595, 1001, 5899, 2065, 2017, 1005, 2128, 4699, 1999, 2108, 10263, 1010, 10373, 2033, 2012, 9389, 4014, 1012, 2123, 8939, 1030, 6111, 1012, 4012]\n"
     ]
    }
   ],
   "source": [
    "# Print the original sentence.\n",
    "print(' Original: ', sentences[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea3b9c89-44de-4609-a518-3052a589908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_len = 0\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For every sentence...\n",
    "#for sent in tqdm(sentences):\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "#    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "#    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "#print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca91cb45-1857-4574-a619-8cbba56273d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/87305 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/nbuser/demo/bert/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████| 87305/87305 [01:14<00:00, 1174.40it/s]\n",
      "100%|███████████████████████████████████| 24288/24288 [00:23<00:00, 1036.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "input_ids_test = []\n",
    "attention_masks_test = []\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 410\n",
    "\n",
    "# For every sentence...\n",
    "for sent in tqdm(sentences):\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = MAX_SENTENCE_LENGTH,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "for sent in tqdm(sentences_test):\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = MAX_SENTENCE_LENGTH,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids_test.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks_test.append(encoded_dict['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3dba6647-c6d0-4fe5-994b-661ab9b432cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids_test = torch.cat(input_ids_test, dim=0)\n",
    "attention_masks_test = torch.cat(attention_masks_test, dim=0)\n",
    "labels_test = torch.tensor(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ae78572-5c97-48b2-b8fd-1635b5745830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  True Statement: Republicans have shown themselves willing to cut millions off their health insurance and eliminate preexisting condition protections for millions more, even in the middle of this public health crisis.\"| Tweet: @RahLeafColorado @lananothony @LAboderin @ToadFur @LPFeinberg @SenSanders The VA is the government running health services we're talking about a bill to have the government run health insurance there's a pretty big difference there. And Medicaid and the VA would be better if they were properly funded, but Republicans like to cut those programs.\n",
      "Token IDs: tensor([  101,  2995,  4861,  1024, 10643,  2031,  3491,  3209,  5627,  2000,\n",
      "         3013,  8817,  2125,  2037,  2740,  5427,  1998, 11027,  3653, 10288,\n",
      "         2923,  2075,  4650, 28548,  2005,  8817,  2062,  1010,  2130,  1999,\n",
      "         1996,  2690,  1997,  2023,  2270,  2740,  5325,  1012,  1000,  1064,\n",
      "         1056, 28394,  2102,  1024,  1030, 10958,  7317,  5243, 11329, 12898,\n",
      "        12173,  2080,  1030, 16554, 17048, 27629,  1030,  6845, 27381,  2378,\n",
      "         1030, 21344, 27942,  1030,  6948,  7959,  2378,  4059,  1030, 12411,\n",
      "         8791, 13375,  1996, 12436,  2003,  1996,  2231,  2770,  2740,  2578,\n",
      "         2057,  1005,  2128,  3331,  2055,  1037,  3021,  2000,  2031,  1996,\n",
      "         2231,  2448,  2740,  5427,  2045,  1005,  1055,  1037,  3492,  2502,\n",
      "         4489,  2045,  1012,  1998, 19960,  5555,  3593,  1998,  1996, 12436,\n",
      "         2052,  2022,  2488,  2065,  2027,  2020,  7919,  6787,  1010,  2021,\n",
      "        10643,  2066,  2000,  3013,  2216,  3454,  1012,   102,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Labels: tensor(0)\n",
      "Original:  False Statement: Says Kamala Harris called Joe Biden a racist during a Democratic presidential debate.| Tweet: @CatFoodMoney @RSBNetwork How can u not like Trump he's not a corrupt politician like Pelosi, or biden he's not a racist like biden.even Harris called him a racist in there debate.look up dumb dumb. U really have no clue. That's sad.just watch foxnews and then you'll no the truth. Your sad man.oh and dumb\n",
      "Token IDs: tensor([  101,  6270,  4861,  1024,  2758, 21911,  2050,  5671,  2170,  3533,\n",
      "         7226,  2368,  1037, 16939,  2076,  1037,  3537,  4883,  5981,  1012,\n",
      "         1064,  1056, 28394,  2102,  1024,  1030,  4937, 14876,  7716,  8202,\n",
      "         3240,  1030, 12667, 24700,  3388,  6198,  2129,  2064,  1057,  2025,\n",
      "         2066,  8398,  2002,  1005,  1055,  2025,  1037, 13593,  3761,  2066,\n",
      "        21877, 10483,  2072,  1010,  2030,  7226,  2368,  2002,  1005,  1055,\n",
      "         2025,  1037, 16939,  2066,  7226,  2368,  1012,  2130,  5671,  2170,\n",
      "         2032,  1037, 16939,  1999,  2045,  5981,  1012,  2298,  2039, 12873,\n",
      "        12873,  1012,  1057,  2428,  2031,  2053,  9789,  1012,  2008,  1005,\n",
      "         1055,  6517,  1012,  2074,  3422,  4419,  2638,  9333,  1998,  2059,\n",
      "         2017,  1005,  2222,  2053,  1996,  3606,  1012,  2115,  6517,  2158,\n",
      "         1012,  2821,  1998, 12873,   102,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Labels: tensor(1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "index = 10\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[index])\n",
    "print('Token IDs:', input_ids[index])\n",
    "print ('Labels:', labels[index])\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences_test[index])\n",
    "print('Token IDs:', input_ids_test[index])\n",
    "print ('Labels:', labels_test[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5374dda6-4c51-4179-a733-e5889f8dd0a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([87305, 410]), torch.Size([87305, 410]), torch.Size([87305]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape, attention_masks.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c66ba4c3-0236-499d-80ca-742b5a2e15a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24288, 410]), torch.Size([24288, 410]), torch.Size([24288]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_test.shape, attention_masks_test.shape, labels_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f2e335c-d319-42a0-a4f8-719cc21c29bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87,305 training samples\n",
      "24,288 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "dataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n",
    "\n",
    "# Create a 80-20 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(len(dataset))\n",
    "val_size = int(len(dataset_test))\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "#train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataset = dataset\n",
    "val_dataset = dataset_test\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f06f5391-9385-4566-9200-92eebe31108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 128\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = RandomSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6bc13d4-68d2-481a-8431-21764fed64ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "402ce2a1-a2bf-43af-86e8-322076484fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08ab44ef-2a35-4b1d-a9a0-83598f033094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbuser/demo/bert/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f73d228-43b8-4a2e-afc4-18df28d17523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 1 as there was incredibly negligiable effects after just 1 epoch.\n",
    "epochs = 1\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "debfe942-a9b0-4990-88d6-50093725dcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "427c284e-eb0e-4e17-a9d0-7f990c56176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46b26116-d8a9-482c-9867-8d1fab53a7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 GPU(s) available.\n",
      "We will use the GPU: NVIDIA RTX 6000 Ada Generation\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda:2\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(2))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49e6e9bf-6f44-439f-bc7a-97b017247a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = 'Truthseeker2023/checkpoints/checkpoint_with_maxlength_410'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c612aa1f-af4e-47df-88b9-97f64475d915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch   100  of    683.    Elapsed: 0:02:22. Training loss. 0.19410964846611023 Num fake examples 6422 Num true examples 6378\n",
      "  Batch   200  of    683.    Elapsed: 0:04:46. Training loss. 0.19548991322517395 Num fake examples 12881 Num true examples 12719\n",
      "  Batch   300  of    683.    Elapsed: 0:07:11. Training loss. 0.24058960378170013 Num fake examples 19214 Num true examples 19186\n",
      "  Batch   400  of    683.    Elapsed: 0:09:35. Training loss. 0.08565721660852432 Num fake examples 25642 Num true examples 25558\n",
      "  Batch   500  of    683.    Elapsed: 0:12:00. Training loss. 0.08568742126226425 Num fake examples 32067 Num true examples 31933\n",
      "  Batch   600  of    683.    Elapsed: 0:14:24. Training loss. 0.13763076066970825 Num fake examples 38450 Num true examples 38350\n",
      "\n",
      "  Average training loss: 0.16\n",
      "  Training epcoh took: 0:16:23\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "  Validation Loss: 0.13\n",
      "  Validation took: 0:01:54\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:18:17 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_fake_examples = 0\n",
    "    total_true_examples = 0\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        #if step > 2000:\n",
    "        if step > 10000:\n",
    "            break\n",
    "        # Progress update every 40 batches.\n",
    "        #if step % 40 == 0 and not step == 0:\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}. Training loss. {:} Num fake examples {:} Num true examples {:}'.format(step, len(train_dataloader), elapsed, train_loss,total_fake_examples, total_true_examples ))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(torch.int64).to(device)\n",
    "        total_fake_examples += (b_labels == 1).sum().item()\n",
    "        total_true_examples += (b_labels == 0).sum().item()\n",
    "        #print (f\"{b_labels.shape=}\")\n",
    "        b_labels_one_hot = torch.nn.functional.one_hot(b_labels, num_classes=2).float()\n",
    "        #print (b_input_ids.shape, b_labels.shape, b_input_mask.shape, b_labels_one_hot.shape, b_labels_one_hot.dtype)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # It returns different numbers of parameters depending on what arguments\n",
    "        # arge given and what flags are set. For our useage here, it returns\n",
    "        # the loss (because we provided labels) and the \"logits\"--the model\n",
    "        # outputs prior to activation.\n",
    "        output = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels_one_hot)\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        train_loss= loss.item()\n",
    "        total_train_loss += train_loss\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "        #print (f\"Training loss\", loss.item())\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    \n",
    "    for step, batch in enumerate(validation_dataloader):\n",
    "        #if step > 2000:\n",
    "        #    break\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(torch.int64).to(device)\n",
    "        b_labels_one_hot = torch.nn.functional.one_hot(b_labels, num_classes=2).float()\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            \n",
    "            output = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels_one_hot)\n",
    "            loss = output.loss\n",
    "            logits = output.logits\n",
    "\n",
    "\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "    #Save model checkpoint\n",
    "    model.save_pretrained(SAVE_DIR)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe6bb68-246e-4093-afb2-cf4363067b8a",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "873821bc-1f66-44fe-acd0-7a312c358621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 410])\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-3.5370,  3.5173]], device='cuda:2', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbuser/demo/bert/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def encode(sentence):\n",
    "    return tokenizer.encode_plus(\n",
    "                        sentence,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 410,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "\n",
    "SENTENCE_INDEX = 5000\n",
    "encoded_dict = encode(sentences[SENTENCE_INDEX])\n",
    "input_id = encoded_dict['input_ids']\n",
    "attention_mask = encoded_dict['attention_mask']\n",
    "print (input_id.shape)\n",
    "model.eval()\n",
    "output = model(\n",
    "            #input_id,\n",
    "            input_id.cuda(2),\n",
    "            token_type_ids=None, \n",
    "            #attention_mask=attention_mask, return_dict=True)\n",
    "            attention_mask=attention_mask.cuda(2), return_dict=True)\n",
    "print (output)\n",
    "print (labels[SENTENCE_INDEX])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fdd7d7-aae7-4b04-afcc-664161c3e215",
   "metadata": {},
   "source": [
    "## Using validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7064487-72c4-43ba-a7ac-128221794554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a534156a-198b-495a-94b8-20b3e28f10a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 410])\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.4296,  0.4381]], device='cuda:2', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "SENTENCE_INDEX = 5000\n",
    "model.eval()\n",
    "#print (torch.unsqueeze(val_dataset[SENTENCE_INDEX][0], dim=0).shape)\n",
    "print (torch.unsqueeze(val_dataset[SENTENCE_INDEX][0].cuda(2), dim=0).shape)\n",
    "output = model(torch.unsqueeze(val_dataset[SENTENCE_INDEX][0].cuda(2), dim=0),\n",
    "#output = model(torch.unsqueeze(val_dataset[SENTENCE_INDEX][0], dim=0),\n",
    "            token_type_ids=None, \n",
    "            #attention_mask=torch.unsqueeze(val_dataset[SENTENCE_INDEX][0],dim=0), return_dict=True)\n",
    "            attention_mask=torch.unsqueeze(val_dataset[SENTENCE_INDEX][0].cuda(2),dim=0), return_dict=True)\n",
    "print (output)\n",
    "print (labels[SENTENCE_INDEX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca1659b2-4081-4161-b331-c0092badb90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_true_labels = []\n",
    "all_predicted_labels = []\n",
    "for step, batch in enumerate(validation_dataloader):\n",
    "    #if step > 5:\n",
    "    #    break\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(torch.int64).to(device)\n",
    "    b_labels_one_hot = torch.nn.functional.one_hot(b_labels, num_classes=2).float()\n",
    "    \n",
    "    # Tell pytorch not to bother with constructing the compute graph during\n",
    "    # the forward pass, since this is only needed for backprop (training).\n",
    "    with torch.no_grad():        \n",
    "\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        # token_type_ids is the same as the \"segment ids\", which \n",
    "        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        \n",
    "        output = model(b_input_ids, \n",
    "                               token_type_ids=None, \n",
    "                               attention_mask=b_input_mask,\n",
    "                               labels=b_labels_one_hot)\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "        #print (logits, b_labels)\n",
    "\n",
    "        # Get predicted class from logits\n",
    "        predicted_class = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        # Append true labels and predicted labels to the lists\n",
    "        all_true_labels.extend(b_labels.cpu().numpy())  # Move to CPU and convert to numpy\n",
    "        all_predicted_labels.extend(predicted_class.cpu().numpy())  # Same for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7d4f170c-91e6-4153-a3cf-872a6550bf69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9691744533281957\n",
      "Recall: 0.9624067342643964\n",
      "F1: 0.9657787377009839\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from cf_matrix import make_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "cf = confusion_matrix(all_true_labels, all_predicted_labels)\n",
    "\n",
    "print(\"Precision: \" + str(precision_score(all_true_labels, all_predicted_labels)))\n",
    "print(\"Recall: \" + str(recall_score(all_true_labels, all_predicted_labels)))\n",
    "print(\"F1: \" + str(f1_score(all_true_labels, all_predicted_labels)))\n",
    "\n",
    "better_cf = make_confusion_matrix(cf, (6,5))\n",
    "plt.savefig('confusion_matrix.png', format='png')  # Saves the plot as a PNG file\n",
    "better_cf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
